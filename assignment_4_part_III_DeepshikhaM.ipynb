{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-YU/M3_Assignment/blob/main/scripts/m3_assignment_part_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III\n",
        "Using the previous two tutorials, please answer the following using an encorder-decoder approach and an LSTM compared approach.\n",
        "\n",
        "Please create a transformer-based classifier for English name classification into male or female.\n",
        "\n",
        "There are several datasets for name for male or female classification. In subseuqent iterations, this could be expanded to included more classifications.\n",
        "\n",
        "Below is the source from NLTK, which only has male and female available but could be used for the purposes of this assignment.\n",
        "\n",
        "```\n",
        "names = nltk.corpus.names\n",
        "names.fileids()\n",
        "['female.txt', 'male.txt']\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "[w for w in male_names if w in female_names]\n",
        "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
        "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
        "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
        "```"
      ],
      "metadata": {
        "id": "QD5ia2HsZpsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKQa86RpY5rH"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "None\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. https://arxiv.org/pdf/2102.03692.pdf\n",
        "2. https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/13-attention.html\n",
        "3. https://towardsdatascience.com/deep-learning-gender-from-name-lstm-recurrent-neural-networks-448d64553044\n",
        "4. https://www.nltk.org/book/ch02.html#sec-lexical-resources"
      ],
      "metadata": {
        "id": "ExMITGgCdQWz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a9E4hC1xE8OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM APPROACH**\n",
        "\n"
      ],
      "metadata": {
        "id": "-jS8vn4BE9RD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://)"
      ],
      "metadata": {
        "id": "PHiDsdXLhbbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the names corpus\n",
        "nltk.download('names')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWYeBTOSziye",
        "outputId": "f2564d49-911f-4445-da0e-6ec64c703d6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load names\n",
        "male_names = nltk.corpus.names.words('male.txt')\n",
        "female_names = nltk.corpus.names.words('female.txt')\n",
        "\n",
        "# Label the names\n",
        "labeled_names = [(name, 0) for name in male_names] + [(name, 1) for name in female_names]\n",
        "\n",
        "# Shuffle the labeled names\n",
        "random.shuffle(labeled_names)\n",
        "\n",
        "# Feature extraction function\n",
        "def name_features(name):\n",
        "    return {'name': name}\n",
        "\n",
        "# Extract features\n",
        "featuresets = [(name_features(name), gender) for (name, gender) in labeled_names]\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_set, test_set = train_test_split(featuresets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Maximum length of names\n",
        "max_name_length = max(len(name) for name, _ in labeled_names)\n",
        "\n",
        "# Prepare data for LSTM\n",
        "def prepare_data_for_lstm(data, max_len):\n",
        "    X, y = [], []\n",
        "    for features, label in data:\n",
        "        name_vec = [ord(char) for char in features['name']]\n",
        "        X.append(name_vec)\n",
        "        y.append(label)\n",
        "    X_padded = pad_sequences(X, maxlen=max_len)\n",
        "    return np.array(X_padded), np.array(y)\n",
        "\n",
        "# Prepare train and test data\n",
        "X_train_lstm, y_train_lstm = prepare_data_for_lstm(train_set, max_name_length)\n",
        "X_test_lstm, y_test_lstm = prepare_data_for_lstm(test_set, max_name_length)\n",
        "\n",
        "# LSTM model\n",
        "lstm_model = Sequential([\n",
        "    Embedding(128, 32, input_length=max_name_length),\n",
        "    LSTM(64),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train_lstm, y_train_lstm, epochs=10, batch_size=64, validation_data=(X_test_lstm, y_test_lstm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd3hZ0yozgXS",
        "outputId": "cba68c0c-68de-465d-e838-83cbedae2cfc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 6s 30ms/step - loss: 0.6187 - accuracy: 0.6571 - val_loss: 0.5445 - val_accuracy: 0.7105\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.4792 - accuracy: 0.7670 - val_loss: 0.4475 - val_accuracy: 0.7823\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 0.4371 - accuracy: 0.7899 - val_loss: 0.4201 - val_accuracy: 0.7967\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 0.4280 - accuracy: 0.7961 - val_loss: 0.4177 - val_accuracy: 0.8030\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.4210 - accuracy: 0.8061 - val_loss: 0.4099 - val_accuracy: 0.8081\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.4176 - accuracy: 0.8088 - val_loss: 0.4128 - val_accuracy: 0.8074\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 0s 5ms/step - loss: 0.4126 - accuracy: 0.8077 - val_loss: 0.4137 - val_accuracy: 0.8055\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.4111 - accuracy: 0.8099 - val_loss: 0.4073 - val_accuracy: 0.8093\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 5ms/step - loss: 0.4073 - accuracy: 0.8088 - val_loss: 0.4034 - val_accuracy: 0.8099\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 0.4038 - accuracy: 0.8112 - val_loss: 0.4020 - val_accuracy: 0.8112\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x791a0c2fe050>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = lstm_model.evaluate(X_test_lstm, y_test_lstm)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Predict on new names\n",
        "def predict_gender(name):\n",
        "    name_vec = np.array([ord(char) for char in name])\n",
        "    name_padded = pad_sequences([name_vec], maxlen=max_name_length)\n",
        "    prediction = lstm_model.predict(name_padded)[0][0]\n",
        "    if prediction >= 0.5:\n",
        "        return 'female'\n",
        "    else:\n",
        "        return 'male'\n",
        "\n",
        "# Test predictions\n",
        "print(predict_gender('John'))  # Expected: male\n",
        "print(predict_gender('Alice'))  # Expected: female\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6DXoO7n_DEb",
        "outputId": "af9dbb50-1838-4809-bd37-3f5d6cfe810d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50/50 [==============================] - 0s 2ms/step - loss: 0.4020 - accuracy: 0.8112\n",
            "Test Loss: 0.402023583650589\n",
            "Test Accuracy: 0.8112019896507263\n",
            "1/1 [==============================] - 0s 376ms/step\n",
            "male\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "female\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aO9L0CvTFN00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENCODER - DECODER APPROACH**"
      ],
      "metadata": {
        "id": "cZlbXkv7FPV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load names\n",
        "male_names = nltk.corpus.names.words('male.txt')\n",
        "female_names = nltk.corpus.names.words('female.txt')\n",
        "\n",
        "# Label the names\n",
        "labeled_names = [(name, 0) for name in male_names] + [(name, 1) for name in female_names]\n",
        "\n",
        "# Shuffle the labeled names\n",
        "random.shuffle(labeled_names)\n",
        "\n",
        "# Feature extraction function\n",
        "def name_features(name):\n",
        "    return {'name': name}\n",
        "\n",
        "# Extract features\n",
        "featuresets = [(name_features(name), gender) for (name, gender) in labeled_names]\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_set, test_set = train_test_split(featuresets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Maximum length of names\n",
        "max_name_length = max(len(name) for name, _ in labeled_names)\n",
        "\n",
        "# Prepare data for encoder and decoder\n",
        "def prepare_data_for_encoder_decoder(data, max_len):\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data = [], [], []\n",
        "    for features, label in data:\n",
        "        name = features['name']\n",
        "        name_vec = [ord(char) for char in name]\n",
        "        encoder_input_data.append(name_vec)\n",
        "        decoder_input_data.append([1] + name_vec)  # Adding start token\n",
        "        decoder_target_data.append(name_vec + [2])  # Adding end token\n",
        "    encoder_input_padded = pad_sequences(encoder_input_data, maxlen=max_len)\n",
        "    decoder_input_padded = pad_sequences(decoder_input_data, maxlen=max_len + 1, padding='post')\n",
        "    decoder_target_padded = pad_sequences(decoder_target_data, maxlen=max_len + 1, padding='post')\n",
        "    return encoder_input_padded, decoder_input_padded, decoder_target_padded\n",
        "\n",
        "# Prepare train and test data for encoder and decoder\n",
        "X_train_enc_dec, X_train_dec_in, y_train_dec_out = prepare_data_for_encoder_decoder(train_set, max_name_length)\n",
        "X_test_enc_dec, X_test_dec_in, y_test_dec_out = prepare_data_for_encoder_decoder(test_set, max_name_length)\n",
        "\n",
        "# Encoder Model\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(128, 32)(encoder_inputs)\n",
        "encoder_lstm = LSTM(64, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder Model\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(128, 32)(decoder_inputs)\n",
        "decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(128, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train_enc_dec, X_train_dec_in], y_train_dec_out,\n",
        "          batch_size=64,\n",
        "          epochs=10,\n",
        "          validation_data=([X_test_enc_dec, X_test_dec_in], y_test_dec_out))\n",
        "\n",
        "# Save the model\n",
        "model.save('encoder_decoder_model.h5')\n",
        "\n",
        "# Define inference encoder model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Define inference decoder model\n",
        "decoder_state_input_h = Input(shape=(64,))\n",
        "decoder_state_input_c = Input(shape=(64,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Function to decode sequence\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = 1  # start token\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        if sampled_token_index == 0:\n",
        "            sampled_char = ''\n",
        "        else:\n",
        "            sampled_char = chr(sampled_token_index)\n",
        "        decoded_sentence += sampled_char\n",
        "        if sampled_char == '\\n' or len(decoded_sentence) > max_name_length:\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# Test predictions\n",
        "def predict_gender(name):\n",
        "    input_seq = np.array([[ord(char) for char in name]])\n",
        "    decoded_name = decode_sequence(input_seq)\n",
        "    return 'female' if decoded_name.strip() == name[1:].strip() else 'male'\n",
        "\n",
        "print(predict_gender('John'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kN07g4ozw0w",
        "outputId": "77595c7f-8cd8-49c4-bb5c-8d9b28bd89a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 5s 23ms/step - loss: 2.0936 - accuracy: 0.5710 - val_loss: 1.4331 - val_accuracy: 0.6204\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 1.3020 - accuracy: 0.6329 - val_loss: 1.2192 - val_accuracy: 0.6578\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 1.1596 - accuracy: 0.6631 - val_loss: 1.1142 - val_accuracy: 0.6730\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 1.0944 - accuracy: 0.6792 - val_loss: 1.0674 - val_accuracy: 0.6795\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 1.0524 - accuracy: 0.6884 - val_loss: 1.0370 - val_accuracy: 0.7016\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 1.0187 - accuracy: 0.6950 - val_loss: 1.0002 - val_accuracy: 0.7041\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 0.9896 - accuracy: 0.6997 - val_loss: 0.9758 - val_accuracy: 0.7054\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 0.9684 - accuracy: 0.7028 - val_loss: 0.9673 - val_accuracy: 0.7067\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 0.9509 - accuracy: 0.7053 - val_loss: 0.9443 - val_accuracy: 0.7080\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 8ms/step - loss: 0.9353 - accuracy: 0.7108 - val_loss: 0.9258 - val_accuracy: 0.7176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 384ms/step\n",
            "1/1 [==============================] - 0s 375ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yM2NOWkhE4Kk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}